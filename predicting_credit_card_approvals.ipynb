{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting credit card approvals using machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To find the project click [here](https://app.datacamp.com/workspace/w/cd940d1b-68ea-458c-8471-5f2e5d8f6beb)\n",
    "### Index (click to redirect to section)\n",
    "- [Loading data into a dataframe](#load-the-data)\n",
    "- [Inspecting the applications](#inspecting-the-applications)\n",
    "- [Handling the missing values](#handling-the-missing-values)\n",
    "- [Processing the data](#prepocessing-the-data)\n",
    "- [Splitting the dataset into train and test sets](#splitting-the-dataset-into-train-and-test-sets)\n",
    "- [Scaling the data](#scaling-the-data)\n",
    "- [Fitting a logistic regression model to the train set](#fitting-a-logistic-regression-model-to-the-train-set)\n",
    "- [Making predictions and evaluating performance](#making-predictions-and-evaluating-performance)\n",
    "- [Grid search to hypertune the model](#grid-search-to-hypertune-the-model)\n",
    "- [Find the best performing model](#find-the-best-performing-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='load-the-data'></a>\n",
    "#### Loading data into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0      1     2  3  4  5  6     7  8  9   10  11  12     13   14  15\n",
      "0  0      1  2.00  3  4  5  6  7.00  8  9  10  11  12     13   14  15\n",
      "1  b  30.83  0.00  u  g  w  v  1.25  t  t   1   f   g  00202    0   +\n",
      "2  a  58.67  4.46  u  g  q  h  3.04  t  t   6   f   g  00043  560   +\n",
      "3  a  24.50  0.50  u  g  q  h  1.50  t  f   0   f   g  00280  824   +\n",
      "4  b  27.83  1.54  u  g  w  v  3.75  t  t   5   t   g  00100    3   +\n"
     ]
    }
   ],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "cc_apps = pd.read_csv('datasets/cc_approvals.data', header=None)\n",
    "\n",
    "# Inspect data\n",
    "print(cc_apps.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='inspecting-the-applications'></a>\n",
    "#### Inspecting the applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               2           7           10             14\n",
      "count  691.000000  691.000000  691.000000     691.000000\n",
      "mean     4.754732    2.230318    2.410999    1015.933430\n",
      "std      4.975661    3.349021    4.868008    5206.465716\n",
      "min      0.000000    0.000000    0.000000       0.000000\n",
      "25%      1.000000    0.165000    0.000000       0.000000\n",
      "50%      2.750000    1.000000    0.000000       5.000000\n",
      "75%      7.165000    2.667500    3.000000     395.000000\n",
      "max     28.000000   28.500000   67.000000  100000.000000\n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 691 entries, 0 to 690\n",
      "Data columns (total 16 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       691 non-null    object \n",
      " 1   1       691 non-null    object \n",
      " 2   2       691 non-null    float64\n",
      " 3   3       691 non-null    object \n",
      " 4   4       691 non-null    object \n",
      " 5   5       691 non-null    object \n",
      " 6   6       691 non-null    object \n",
      " 7   7       691 non-null    float64\n",
      " 8   8       691 non-null    object \n",
      " 9   9       691 non-null    object \n",
      " 10  10      691 non-null    int64  \n",
      " 11  11      691 non-null    object \n",
      " 12  12      691 non-null    object \n",
      " 13  13      691 non-null    object \n",
      " 14  14      691 non-null    int64  \n",
      " 15  15      691 non-null    object \n",
      "dtypes: float64(2), int64(2), object(12)\n",
      "memory usage: 86.5+ KB\n",
      "None\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0     0\n",
       "1     0\n",
       "2     0\n",
       "3     0\n",
       "4     0\n",
       "5     0\n",
       "6     0\n",
       "7     0\n",
       "8     0\n",
       "9     0\n",
       "10    0\n",
       "11    0\n",
       "12    0\n",
       "13    0\n",
       "14    0\n",
       "15    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print summary statistics\n",
    "cc_apps_description = cc_apps.describe()\n",
    "print(cc_apps_description)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Print DataFrame information\n",
    "cc_apps_info = cc_apps.info()\n",
    "print(cc_apps_info)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Inspect missing values in the dataset\n",
    "cc_apps.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='handling-the-missing-values'></a>\n",
    "#### Handling the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     0\n",
      "1     0\n",
      "2     0\n",
      "3     0\n",
      "4     0\n",
      "5     0\n",
      "6     0\n",
      "7     0\n",
      "8     0\n",
      "9     0\n",
      "10    0\n",
      "11    0\n",
      "12    0\n",
      "13    0\n",
      "14    0\n",
      "15    0\n",
      "dtype: int64\n",
      "0     12\n",
      "1     12\n",
      "2      0\n",
      "3      6\n",
      "4      6\n",
      "5      9\n",
      "6      9\n",
      "7      0\n",
      "8      0\n",
      "9      0\n",
      "10     0\n",
      "11     0\n",
      "12     0\n",
      "13    13\n",
      "14     0\n",
      "15     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Import numpy\n",
    "import numpy as np\n",
    "\n",
    "# Inspect missing values in the dataset\n",
    "print(cc_apps.isna().sum())\n",
    "\n",
    "# Replace the '?'s with NaN\n",
    "cc_apps = cc_apps.replace('?', np.nan)\n",
    "\n",
    "# Inspect the missing values again\n",
    "print(cc_apps.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     12\n",
      "1     12\n",
      "2      0\n",
      "3      6\n",
      "4      6\n",
      "5      9\n",
      "6      9\n",
      "7      0\n",
      "8      0\n",
      "9      0\n",
      "10     0\n",
      "11     0\n",
      "12     0\n",
      "13    13\n",
      "14     0\n",
      "15     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Impute the missing values with mean imputation\n",
    "cc_apps = cc_apps.fillna(cc_apps.mean())\n",
    "\n",
    "# Count the number of NaNs in the dataset to verify\n",
    "print(cc_apps.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0\n",
       "1     0\n",
       "2     0\n",
       "3     0\n",
       "4     0\n",
       "5     0\n",
       "6     0\n",
       "7     0\n",
       "8     0\n",
       "9     0\n",
       "10    0\n",
       "11    0\n",
       "12    0\n",
       "13    0\n",
       "14    0\n",
       "15    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iterate over each column of cc_apps\n",
    "for col in cc_apps.columns:\n",
    "    # Check if the column is of object type\n",
    "    if cc_apps[col].dtype == 'object':\n",
    "        # Impute with the most frequent value\n",
    "        cc_apps = cc_apps.fillna(cc_apps[col].value_counts().index[0])\n",
    "\n",
    "# Count the number of NaNs in the dataset and print the counts to verify\n",
    "cc_apps.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='prepocessing-the-data'></a>\n",
    "#### Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LabelEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Instantiate LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Iterate over all the values of each column and extract their dtypes\n",
    "for col in cc_apps.columns.to_numpy():\n",
    "    # Compare if the dtype is object\n",
    "    if cc_apps[col].dtypes == 'object':\n",
    "        # Use LabelEncoder to do the numeric transformation\n",
    "        cc_apps[col] = le.fit_transform(cc_apps[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='splitting-the-dataset-into-train-and-test-sets'></a>\n",
    "#### Splitting the dataset into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Drop the features 11 and 13 and convert the DataFrame to a NumPy array\n",
    "cc_apps = cc_apps.drop([11, 13], axis=1)\n",
    "cc_apps = cc_apps.to_numpy()\n",
    "\n",
    "# Segregate features and labels into separate variables\n",
    "X, y = cc_apps[:, 0:-1], cc_apps[:, -1]\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='scaling-the-data'></a>\n",
    "#### Scaling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Instantiate MinMaxScaler and use it to rescale X_train and X_test\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "rescaledX_train = scaler.fit_transform(X_train)\n",
    "rescaledX_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='fitting-a-logistic-regression-model-to-the-train-set'></a>\n",
    "#### Fitting a logistic regression model to the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Instantiate a LogisticRegression classifier with default parameter values\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Fit logreg to the train set\n",
    "logreg.fit(rescaledX_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='making-predictions-and-evaluating-performance'></a>\n",
    "#### Making predictions and evaluating performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier:  0.6550218340611353\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.83      0.81       103\n",
      "         1.0       0.86      0.81      0.83       126\n",
      "\n",
      "    accuracy                           0.82       229\n",
      "   macro avg       0.82      0.82      0.82       229\n",
      "weighted avg       0.82      0.82      0.82       229\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Use logreg to predict instances from the test set and store it\n",
    "y_pred = logreg.predict(rescaledX_test)\n",
    "\n",
    "# Get the accuracy score of logreg model and print it\n",
    "print(\"Accuracy of logistic regression classifier: \",\n",
    "      logreg.score(X_test, y_test))\n",
    "\n",
    "# Print the confusion matrix of the logreg model\n",
    "confusion_matrix(y_test, y_pred)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='grid-search-to-hypertune-the-model'></a>\n",
    "#### Grid search to hypertune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the grid of values for tol and max_iter\n",
    "tol = [0.01, 0.001, 0.0001]\n",
    "max_iter = [100, 150, 200]\n",
    "\n",
    "# Create a dictionary where tol and max_iter are keys and the lists of their values are corresponding values\n",
    "param_grid = dict({'tol': tol, 'max_iter': max_iter})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='find-the-best-performing-model'></a>\n",
    "#### Find the best performing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:666: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.849828 using {'max_iter': 100, 'tol': 0.01}\n"
     ]
    }
   ],
   "source": [
    "# Instantiate GridSearchCV with the required parameters\n",
    "grid_model = GridSearchCV(estimator=logreg, param_grid=param_grid, cv=5)\n",
    "\n",
    "# Use scaler to rescale X and assign it to rescaledX\n",
    "rescaledX = scaler.fit_transform(X)\n",
    "\n",
    "# Fit data to grid_model\n",
    "grid_model_result = grid_model.fit(rescaledX, y)\n",
    "\n",
    "# Summarize results\n",
    "best_score, best_params = grid_model_result.best_score_, grid_model_result.best_params_\n",
    "print(\"Best: %f using %s\" % (best_score, best_params))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
